{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "easy_pred.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SaHUetwiBGa1LFZeZYzTbZv-P8-0Sgic",
      "authorship_tag": "ABX9TyM3pvum+eG/Iq9+hqcXZxOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhu1096/easy_pred/blob/main/easy_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hLCMTwt0JZo"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "try:\n",
        "    from skopt import BayesSearchCV\n",
        "    from skopt.space import Real, Categorical, Integer\n",
        "    from skopt.callbacks import DeadlineStopper, VerboseCallback\n",
        "    from scipy.stats import randint as sp_randint\n",
        "    from scipy.stats import uniform as sp_uniform\n",
        "except:\n",
        "    !pip install scikit-optimize\n",
        "    from skopt import BayesSearchCV\n",
        "    from skopt.space import Real, Categorical, Integer\n",
        "    from skopt.callbacks import DeadlineStopper, VerboseCallback\n",
        "    from scipy.stats import randint as sp_randint\n",
        "    from scipy.stats import uniform as sp_uniform\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold,KFold,RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score,make_scorer,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "except:\n",
        "    !pip install catboost\n",
        "    from catboost import CatBoostClassifier\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "except:\n",
        "    !pip install lightgbm\n",
        "    from lightgbm import LGBMClassifier\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "class easy_pred:\n",
        "  def __init__(self,train_lib=None,\n",
        "               test_lib=None,target_column=None,\n",
        "               ignore_cols=None,tuning=True,\n",
        "               c=True,l=True,x=True,r=True):\n",
        "    \"\"\" \n",
        "        train_lib = file path\n",
        "        test_lib  = test file path\n",
        "        target_column = y\n",
        "        ignore_cols   = like ID\n",
        "        c = Catboost classifier\n",
        "        l = LGBM classifier\n",
        "        x = XGB classifier\n",
        "        r = Random forest classifier\n",
        "    \"\"\"\n",
        "    self.train_lib     = train_lib\n",
        "    self.test_lib      = test_lib\n",
        "    self.target_column = target_column\n",
        "    self.ignore_cols   = ignore_cols\n",
        "    self.tuning        = tuning\n",
        "    self.c             = c\n",
        "    self.l             = l\n",
        "    self.x             = x\n",
        "    self.r             = r  \n",
        "\n",
        "  def read_preprocess(self):    \n",
        "    train = pd.read_csv(self.train_lib)\n",
        "    train['train'] = 1\n",
        "    if self.test_lib != None:\n",
        "      test = pd.read_csv(self.test_lib)  \n",
        "      test['train'] = 0\n",
        "      df = pd.concat([train,test],axis=0).reset_index(drop=True)\n",
        "    else:\n",
        "      print('No test data passed')\n",
        "      df = train\n",
        "     \n",
        "    if self.ignore_cols==None:\n",
        "      print('No ignore columns passed')\n",
        "    else:\n",
        "      df = df.drop(self.ignore_cols,axis=1)\n",
        "\n",
        "    for i in df.columns:\n",
        "      if df[i].dtype == 'object':\n",
        "        df[i].fillna(value=i, inplace=True)\n",
        "        lb = LabelEncoder()\n",
        "        df[i]=lb.fit_transform(df[i])\n",
        "      elif (i!='train' and i!=self.target_column):\n",
        "        df[i].fillna(value=df[i].mean(), inplace=True)\n",
        "        temp = np.array(df[i]).reshape(-1,1)\n",
        "        sc=StandardScaler()\n",
        "        df[i] = sc.fit_transform(temp)\n",
        "    if df[self.target_column].dtype == 'object':\n",
        "      lb = LabelEncoder()\n",
        "      df[self.target_column]=lb.fit_transform(df[self.target_column])\n",
        "\n",
        "    print('********************************************************************')\n",
        "    print('**********************Preprocessing completed***********************')\n",
        "    print('********************************************************************')\n",
        "     \n",
        "    return df\n",
        "  #  catboosting\n",
        "  def cat_tuning_training(self,df):\n",
        "    start = time()     \n",
        "    X = df[df['train']==1]\n",
        "    y = df[df['train']==1][self.target_column]\n",
        "    X.drop([self.target_column],axis=1,inplace=True)\n",
        "    roc_auc = make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True)\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.50, random_state=1503,shuffle=True)\n",
        "    # Defining your search space\n",
        "    search_spaces   = {'iterations': Integer(10, 1000),\n",
        "                        'depth': Integer(1, 8),\n",
        "                        'learning_rate': Real(0.009, 0.01, 'log-uniform'),\n",
        "                        'random_strength': Real(1e-9, 10, 'log-uniform'),\n",
        "                        'bagging_temperature': Real(0.0, 1.0),\n",
        "                        'border_count': Integer(1, 255),\n",
        "                        'l2_leaf_reg': Integer(2, 30),\n",
        "                        'scale_pos_weight':Real(0.01, 1.0, 'uniform')}\n",
        "\n",
        "    clf = CatBoostClassifier(thread_count=2,\n",
        "                            loss_function='Logloss',                        \n",
        "                            od_type = 'Iter',\n",
        "                            verbose= False\n",
        "                            )\n",
        "    # Setting up BayesSearchCV\n",
        "    opt = BayesSearchCV(clf,\n",
        "                        search_spaces,\n",
        "                        scoring=roc_auc,\n",
        "                        cv=skf,\n",
        "                        n_iter=100,\n",
        "                        n_jobs=-1,\n",
        "                        return_train_score=False,\n",
        "                        refit=True,\n",
        "                        optimizer_kwargs={'base_estimator': 'GP'},\n",
        "                        random_state=42)\n",
        "    print('********************************************************************')\n",
        "    print('*******************CatBoost fine tuning started*********************')\n",
        "    print('********************************************************************')     \n",
        "    if self.tuning == True:\n",
        "      opt.fit(X_train, y_train, callback=[VerboseCallback(100),DeadlineStopper(60*10)]) \n",
        "    else:\n",
        "      print('Tune bypassing-Cat')\n",
        "      catb = CatBoostClassifier(n_estimators = 3000,\n",
        "                        learning_rate = 0.02,\n",
        "                        rsm = 0.4,\n",
        "                        random_state=2054)\n",
        "      catb.fit(X,y,verbose=0)\n",
        "      probs_cat_test = catb.predict_proba(X)[:, 1]\n",
        "      print(confusion_matrix(y,probs_cat_test>0.5)) \n",
        "      print('CAT ' ,roc_auc_score(df[df['train']==1][self.target_column],probs_cat_test))\n",
        "      temp = time() - start\n",
        "      temp = temp/60\n",
        "      print('***********CatBoost Completed - Total time {a} Mins****************'.format(a=round(temp,2)))\n",
        "      return catb\n",
        "    print('**************CatBoost fine tuning Completed************************')         \n",
        "    d=pd.DataFrame(opt.cv_results_)\n",
        "    best_score = opt.best_score_\n",
        "    best_score_std = d.iloc[opt.best_index_].std_test_score\n",
        "    best_params = opt.best_params_  \n",
        "    catb = CatBoostClassifier(**best_params,od_type='Iter',one_hot_max_size=10)\n",
        "    catb.fit(X,y,verbose=0)\n",
        "    probs_cat_test = catb.predict_proba(X)[:, 1]\n",
        "    print(confusion_matrix(y,probs_cat_test>0.5)) \n",
        "    print('CAT ' ,roc_auc_score(df[df['train']==1][self.target_column],probs_cat_test))\n",
        "    temp = time() - start\n",
        "    temp = temp/60\n",
        "    print('***********CatBoost Completed - Total time {a} Mins****************'.format(a=round(temp,2)))\n",
        "    \n",
        "    return catb\n",
        "  # LGB\n",
        "  def lgb_tuning_training(self,df):\n",
        "    start = time()\n",
        "    X = df[df['train']==1]\n",
        "    y = df[df['train']==1][self.target_column]\n",
        "    X.drop([self.target_column],axis=1,inplace=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.50, random_state=1503,shuffle=True)\n",
        "    fit_params={\"early_stopping_rounds\":30, \n",
        "              \"eval_metric\" : 'auc', \n",
        "              \"eval_set\" : [(X_test,y_test)],\n",
        "              'eval_names': ['valid'],\"eval_metric\" : 'auc',\n",
        "              'categorical_feature': 'auto'}\n",
        "\n",
        "    param_test ={'num_leaves': sp_randint(6, 50), \n",
        "              'min_child_samples': sp_randint(100, 500), \n",
        "              'min_child_weight': [ 1e-2, 1e-1, 1, 1e1, 1e2],\n",
        "              'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
        "              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
        "              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10],\n",
        "              'reg_lambda': [0, 1e-1, 1, 5, 10]}\n",
        "    \n",
        "    clf = LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_estimators=500,verbose=0)\n",
        "    gs = RandomizedSearchCV(\n",
        "      estimator=clf, param_distributions=param_test, \n",
        "      n_iter=100,\n",
        "      scoring='roc_auc',\n",
        "      cv=3, verbose=1,\n",
        "      random_state=314)\n",
        "    print('********************************************************************')\n",
        "    print('*********************LBGM fine tuning started***********************')\n",
        "    print('********************************************************************') \n",
        "    if self.tuning == True:\n",
        "      gs.fit(X_train, y_train,**fit_params,verbose=False)\n",
        "    else:\n",
        "      print('Tune bypassing-LGB')\n",
        "      clf_final = LGBMClassifier(n_estimators = 200,\n",
        "                          learning_rate = 0.05,\n",
        "                          colsample_bytree = 0.5\n",
        "                          )\n",
        "      clf_final.fit(X, y,**fit_params,verbose=False)\n",
        "      probs_lgb_test = clf_final.predict_proba(X)[:, 1]\n",
        "      print(confusion_matrix(y,probs_lgb_test>0.5))\n",
        "      print('LGB ' ,roc_auc_score(df[df['train']==1][self.target_column],probs_lgb_test))\n",
        "      temp = time() - start\n",
        "      temp = temp/60\n",
        "      print('*************CatBoost Completed - Total time {a} Mins***************'.format(a=round(temp,2)))\n",
        "      return clf_final\n",
        "    print('*********************LBGM fine tuning completed*********************')\n",
        "    clf_final = LGBMClassifier(**gs.best_estimator_.get_params())\n",
        "    #set optimal parameters\n",
        "    clf_final.set_params(**gs.best_params_)\n",
        "\n",
        "    #Train the final model with learning rate decay\n",
        "    clf_final.fit(X, y,**fit_params,verbose=False)\n",
        "    probs_lgb_test = clf_final.predict_proba(X)[:, 1]\n",
        "    print(confusion_matrix(y,probs_lgb_test>0.5))\n",
        "    print('LGB ' ,roc_auc_score(df[df['train']==1][self.target_column],probs_lgb_test))\n",
        "    temp = time() - start\n",
        "    temp = temp/60\n",
        "    print('************CatBoost Completed - Total time {a} Mins***************'.format(a=round(temp,2)))\n",
        "    \n",
        "    return clf_final\n",
        "  # XGB  \n",
        "  def xgb_traning(self,df):\n",
        "    start = time()\n",
        "    X = df[df['train']==1]\n",
        "    y = df[df['train']==1][self.target_column]\n",
        "    X.drop([self.target_column],axis=1,inplace=True)\n",
        "    print('********************************************************************')\n",
        "    print('**********************Fitting XGB started***************************')\n",
        "    print('********************************************************************') \n",
        "    xgb = XGBClassifier(learning_rate =0.02, n_estimators=140, max_depth=4,\n",
        "                      min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
        "                      objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\n",
        "    xgb.fit(X,y)\n",
        "    probs_xgb_test = xgb.predict_proba(X)[:, 1]\n",
        "    print(confusion_matrix(y,probs_xgb_test>0.5))\n",
        "    print('XGB ' ,roc_auc_score(df[df['train']==1][self.target_column],probs_xgb_test))\n",
        "    temp = time() - start\n",
        "    temp = temp/60\n",
        "    print('**************XGB Completed - Total time {a} Mins******************'.format(a=round(temp,2)))\n",
        "\n",
        "    return xgb\n",
        "  # random forest\n",
        "  def raf_training(self,df):\n",
        "    start = time() \n",
        "    X = df[df['train']==1]\n",
        "    y = df[df['train']==1][self.target_column]\n",
        "    X.drop([self.target_column],axis=1,inplace=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.50, random_state=1503,shuffle=True)\n",
        "    n_estimators = [10,50,100,150]\n",
        "    max_depth = [5, 8, 15, 25, 30]\n",
        "    min_samples_split = [2, 5, 10, 15, 100]\n",
        "    min_samples_leaf = [1, 2, 5, 10] \n",
        "    bootstrap = [True, False]\n",
        "    r_grid = {'n_estimators': n_estimators,\n",
        "              'min_samples_split' : min_samples_split, \n",
        "              'min_samples_leaf' : min_samples_leaf,\n",
        "              'max_depth': max_depth,\n",
        "              'bootstrap': bootstrap}\n",
        "    rfr = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions=r_grid,\n",
        "                                    n_iter = 20, scoring='neg_mean_absolute_error',\n",
        "                                    cv = 3, verbose=2,\n",
        "                                    random_state=42, n_jobs=-1,\n",
        "                                    return_train_score=True)\n",
        "    print('********************************************************************')\n",
        "    print('***************Random forest fine Tuning started********************')\n",
        "    print('********************************************************************') \n",
        "    if self.tuning == True:\n",
        "      rfr.fit(X_train, y_train)\n",
        "    else:\n",
        "      print('Tune bypassing-RAF')\n",
        "      raf = RandomForestClassifier(n_estimators =200, criterion = 'entropy')\n",
        "      raf.fit(X, y)\n",
        "      probs_raf_test = raf.predict_proba(X)[:, 1]\n",
        "      print(confusion_matrix(y,probs_raf_test>0.5))\n",
        "      print('RAF ' ,roc_auc_score(df[df['train']==1][self.target_column],probs_raf_test))\n",
        "      temp = time() - start\n",
        "      temp = temp/60\n",
        "      print('***********Random F Completed - Total time {a} Mins****************'.format(a=round(temp,2)))\n",
        "      return raf\n",
        "    print('***************Random forest fine Tuning completed******************')\n",
        "    raf = RandomForestClassifier(**rfr.best_params_)\n",
        "    raf.fit(X, y)\n",
        "    probs_raf_test = raf.predict_proba(X)[:, 1]\n",
        "    print(confusion_matrix(y,probs_raf_test>0.5))\n",
        "    print('RAF ' ,roc_auc_score(df[df['train']==1][self.target_column],probs_raf_test))\n",
        "    temp = time() - start\n",
        "    temp = temp/60\n",
        "    print('***********Random F Completed - Total time {a} Mins****************'.format(a=round(temp,2)))\n",
        "\n",
        "    return raf\n",
        "\n",
        "  # predicting\n",
        "  def predicting(self,df,catb,lgb,xgb,raf,catb_e):\n",
        "    X = df[df['train']==0]\n",
        "    X.drop([self.target_column],axis=1,inplace=True)\n",
        "    catb_pred,lgb_pred,xgb_pred,raf_pred=[],[],[],[]\n",
        "    df_e = pd.DataFrame()\n",
        "    if self.c == True:\n",
        "      catb_pred = catb.predict_proba(X)[:, 1]\n",
        "      df_e['catb'] = catb_pred\n",
        "    if self.l == True:\n",
        "      lgb_pred  = lgb.predict_proba(X)[:, 1]\n",
        "      df_e['lgb']  = lgb_pred\n",
        "    if self.x == True:\n",
        "      xgb_pred  = xgb.predict_proba(X)[:, 1]\n",
        "      df_e['xgb']  = xgb_pred\n",
        "    if self.r == True:\n",
        "      raf_pred  = raf.predict_proba(X)[:, 1]\n",
        "      df_e['raf']  = raf_pred\n",
        "    \n",
        "    y_pred = catb_e.predict_proba(df_e)[:, 1]\n",
        "    \n",
        "    return X,y_pred\n",
        "\n",
        "  def ensembling(self,catb_pred,lgb_pred,xgb_pred,raf_pred,df):\n",
        "    print('***********************Ensembling started***************************') \n",
        "    df_e = pd.DataFrame()\n",
        "    if self.c == True:   \n",
        "      df_e['catb'] = catb_pred\n",
        "    if self.l == True:\n",
        "      df_e['lgb']  = lgb_pred\n",
        "    if self.x == True:\n",
        "      df_e['xgb']  = xgb_pred\n",
        "    if self.r == True:\n",
        "      df_e['raf']  = raf_pred\n",
        "    y = df[df['train']==1][self.target_column]\n",
        "    catb_e = CatBoostClassifier(n_estimators = 3000,\n",
        "                        learning_rate = 0.02,\n",
        "                        rsm = 0.4,\n",
        "                        random_state=2054)\n",
        "    \n",
        "    catb_e.fit(df_e,y,early_stopping_rounds=30,verbose=0)\n",
        "    probs_ens_test = catb_e.predict_proba(df_e)[:, 1]\n",
        "    print(confusion_matrix(y,probs_ens_test>0.5))\n",
        "    print(roc_auc_score(y, probs_ens_test))\n",
        "    print('**********************Ensembling completed**************************')\n",
        "    return catb_e\n",
        "\n",
        "  # Ensembling\n",
        "  def train_ensemble(self):\n",
        "    \n",
        "    start_tot = time()\n",
        "    df = self.read_preprocess()\n",
        "    X = df[df['train']==1]\n",
        "    y = df[df['train']==1][self.target_column]\n",
        "    X.drop([self.target_column],axis=1,inplace=True)\n",
        "    catb,lgb,xgb,raf,catb_pred,lgb_pred,xgb_pred,raf_pred=[],[],[],[],[],[],[],[]\n",
        "\n",
        "    if self.c == True:\n",
        "      catb = self.cat_tuning_training(df)\n",
        "      catb_pred = catb.predict_proba(X)[:, 1]\n",
        "    if self.l == True:\n",
        "      lgb  = self.lgb_tuning_training(df)\n",
        "      lgb_pred  = lgb.predict_proba(X)[:, 1]\n",
        "    if self.x == True:\n",
        "      xgb  = self.xgb_traning(df)\n",
        "      xgb_pred  = xgb.predict_proba(X)[:, 1]\n",
        "    if self.r == True:\n",
        "      raf  = self.raf_training(df)\n",
        "      raf_pred  = raf.predict_proba(X)[:, 1]\n",
        "      \n",
        "    catb_e = self.ensembling(catb_pred,lgb_pred,xgb_pred,raf_pred,df)\n",
        "    X,y_pred = self.predicting(df,catb,lgb,xgb,raf,catb_e)\n",
        "    temp = time() - start_tot\n",
        "    temp = temp/60\n",
        "    models = [catb,lgb,xgb,raf,catb_e]\n",
        "    print('**********************Completed - Total time {a} ********************'.format(a=temp))\n",
        "    \n",
        "    return df,y_pred,models\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    
      ]
    }
  ]
}
